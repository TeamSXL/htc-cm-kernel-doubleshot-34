--- drivers/gpu/ion/ion_cp_heap.c
+++ drivers/gpu/ion/ion_cp_heap.c
@@ -23,11 +23,12 @@
 #include <linux/mm.h>
 #include <linux/scatterlist.h>
 #include <linux/slab.h>
+#include <linux/vmalloc.h>
 #include <linux/memory_alloc.h>
 #include <linux/seq_file.h>
-#include <linux/fmem.h>
 #include <linux/iommu.h>
 #include <linux/dma-mapping.h>
+#include <trace/events/kmem.h>
 
 #include <asm/mach/map.h>
 
@@ -68,8 +69,6 @@
  *		user space.
  * @iommu_iova: saved iova when mapping full heap at once.
  * @iommu_partition: partition used to map full heap.
- * @reusable: indicates if the memory should be reused via fmem.
- * @reserved_vrange: reserved virtual address range for use with fmem
  * @iommu_map_all:	Indicates whether we should map whole heap into IOMMU.
  * @iommu_2x_map_domain: Indicates the domain to use for overmapping.
  * @has_outer_cache:    set to 1 if outer cache is used, 0 otherwise.
@@ -102,7 +100,7 @@
 	size_t heap_size;
 	dma_addr_t handle;
 	int cma;
-	int disallow_non_secure_allocation;
+	int allow_non_secure_allocation;
 };
 
 enum {
@@ -110,17 +108,8 @@
 	HEAP_PROTECTED = 1,
 };
 
-#define DMA_ALLOC_RETRIES	5
+#define DMA_ALLOC_TRIES	5
 
-static int ion_cp_protect_mem(unsigned int phy_base, unsigned int size,
-			unsigned int permission_type, int version,
-			void *data);
-
-static int ion_cp_unprotect_mem(unsigned int phy_base, unsigned int size,
-				unsigned int permission_type, int version,
-				void *data);
-
-#if 0
 static int allocate_heap_memory(struct ion_heap *heap)
 {
 	struct device *dev = heap->priv;
@@ -135,14 +124,16 @@
 	if (cp_heap->cpu_addr)
 		return 0;
 
-	while (!cp_heap->cpu_addr && (++tries < DMA_ALLOC_RETRIES)) {
+	while (!cp_heap->cpu_addr && (++tries < DMA_ALLOC_TRIES)) {
 		cp_heap->cpu_addr = dma_alloc_attrs(dev,
 						cp_heap->heap_size,
 						&(cp_heap->handle),
 						0,
 						&attrs);
-		if (!cp_heap->cpu_addr)
+		if (!cp_heap->cpu_addr) {
+			trace_ion_cp_alloc_retry(tries);
 			msleep(20);
+		}
 	}
 
 	if (!cp_heap->cpu_addr)
@@ -169,7 +160,7 @@
 out:
 	return ION_CP_ALLOCATE_FAIL;
 }
-#endif
+
 static void free_heap_memory(struct ion_heap *heap)
 {
 	struct device *dev = heap->priv;
@@ -196,19 +187,12 @@
 	return cp_heap->kmap_cached_count + cp_heap->kmap_uncached_count;
 }
 
-#if 0
 static int ion_on_first_alloc(struct ion_heap *heap)
 {
 	struct ion_cp_heap *cp_heap =
 		container_of(heap, struct ion_cp_heap, heap);
 	int ret_value;
 
-	if (cp_heap->reusable) {
-		ret_value = fmem_set_state(FMEM_C_STATE);
-		if (ret_value)
-			return 1;
-	}
-
 	if (cp_heap->cma) {
 		ret_value = allocate_heap_memory(heap);
 		if (ret_value)
@@ -216,18 +200,12 @@
 	}
 	return 0;
 }
-#endif
 
 static void ion_on_last_free(struct ion_heap *heap)
 {
 	struct ion_cp_heap *cp_heap =
 		container_of(heap, struct ion_cp_heap, heap);
 
-	if (cp_heap->reusable)
-		if (fmem_set_state(FMEM_T_STATE) != 0)
-			pr_err("%s: unable to transition heap to T-state\n",
-				__func__);
-
 	if (cp_heap->cma)
 		free_heap_memory(heap);
 }
@@ -245,31 +223,6 @@
 
 	if (atomic_inc_return(&cp_heap->protect_cnt) == 1) {
 		/* Make sure we are in C state when the heap is protected. */
-		if (cp_heap->reusable && !cp_heap->allocated_bytes) {
-			ret_value = fmem_set_state(FMEM_C_STATE);
-			if (ret_value)
-				goto out;
-		}
-
-		ret_value = ion_cp_protect_mem(cp_heap->secure_base,
-				cp_heap->secure_size, cp_heap->permission_type,
-				version, data);
-		if (ret_value) {
-			pr_err("Failed to protect memory for heap %s - "
-				"error code: %d\n", heap->name, ret_value);
-
-			if (cp_heap->reusable && !cp_heap->allocated_bytes) {
-				if (fmem_set_state(FMEM_T_STATE) != 0)
-					pr_err("%s: unable to transition heap to T-state\n",
-						__func__);
-			}
-			atomic_dec(&cp_heap->protect_cnt);
-		} else {
-			cp_heap->heap_protected = HEAP_PROTECTED;
-			pr_debug("Protected heap %s @ 0x%lx\n",
-				heap->name, cp_heap->base);
-		}
-#if 0
 		if (!cp_heap->allocated_bytes)
 			if (ion_on_first_alloc(heap))
 				goto out;
@@ -287,10 +240,9 @@
 			atomic_dec(&cp_heap->protect_cnt);
 		} else {
 			cp_heap->heap_protected = HEAP_PROTECTED;
-			pr_debug("Protected heap %s @ 0x%lx\n",
-				heap->name, cp_heap->base);
+			pr_debug("Protected heap %s @ 0x%pa\n",
+				heap->name, &cp_heap->base);
 		}
-#endif
 	}
 out:
 	pr_debug("%s: protect count is %d\n", __func__,
@@ -336,8 +288,8 @@
 				      unsigned long flags)
 {
 	unsigned long offset;
-	unsigned long secure_allocation = flags & ION_SECURE;
-	unsigned long force_contig = flags & ION_FORCE_CONTIGUOUS;
+	unsigned long secure_allocation = flags & ION_FLAG_SECURE;
+	unsigned long force_contig = flags & ION_FLAG_FORCE_CONTIGUOUS;
 
 	struct ion_cp_heap *cp_heap =
 		container_of(heap, struct ion_cp_heap, heap);
@@ -351,31 +303,25 @@
 	}
 
 	if (!force_contig && !secure_allocation &&
-	     cp_heap->disallow_non_secure_allocation) {
+	     !cp_heap->allow_non_secure_allocation) {
 		mutex_unlock(&cp_heap->lock);
 		pr_debug("%s: non-secure allocation disallowed from this heap\n",
 			__func__);
 		return ION_CP_ALLOCATE_FAIL;
 	}
 
-	if (secure_allocation &&
-	    (cp_heap->umap_count > 0 || cp_heap->kmap_cached_count > 0)) {
-		mutex_unlock(&cp_heap->lock);
-		pr_err("ION cannot allocate secure memory from heap with "
-			"outstanding mappings: User space: %lu, kernel space "
-			"(cached): %lu\n", cp_heap->umap_count,
-					   cp_heap->kmap_cached_count);
-		return ION_CP_ALLOCATE_FAIL;
-	}
-
-	if (cp_heap->reusable && !cp_heap->allocated_bytes) {
-		if (fmem_set_state(FMEM_C_STATE) != 0) {
-			mutex_unlock(&cp_heap->lock);
-			return ION_RESERVED_ALLOCATE_FAIL;
-		}
+	/*
+	 * The check above already checked for non-secure allocations when the
+	 * heap is protected. HEAP_PROTECTED implies that this must be a secure
+	 * allocation. If the heap is protected and there are userspace or
+	 * cached kernel mappings, something has gone wrong in the security
+	 * model.
+	 */
+	if (cp_heap->heap_protected == HEAP_PROTECTED) {
+		BUG_ON(cp_heap->umap_count != 0);
+		BUG_ON(cp_heap->kmap_cached_count != 0);
 	}
 
-#if 0
 	/*
 	 * if this is the first reusable allocation, transition
 	 * the heap
@@ -386,7 +332,6 @@
 			return ION_RESERVED_ALLOCATE_FAIL;
 		}
 
-#endif
 	cp_heap->allocated_bytes += size;
 	mutex_unlock(&cp_heap->lock);
 
@@ -605,33 +550,35 @@
 			return NULL;
 		}
 
-		if (cp_heap->reusable) {
-			ret_value = ion_map_fmem_buffer(buffer, cp_heap->base,
-				cp_heap->reserved_vrange, buffer->flags);
-		} else if (cp_heap->cma) {
+		if (cp_heap->cma) {
 			int npages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;
 			struct page **pages = vmalloc(
 						sizeof(struct page *) * npages);
 			int i;
 			pgprot_t pgprot;
 
+			if (!pages) {
+				mutex_unlock(&cp_heap->lock);
+				return ERR_PTR(-ENOMEM);
+			}
+
 			if (ION_IS_CACHED(buffer->flags))
 				pgprot = PAGE_KERNEL;
 			else
 				pgprot = pgprot_writecombine(PAGE_KERNEL);
 
 			for (i = 0; i < npages; i++) {
-				pages[i] = phys_to_page(buffer->priv_phys +
+				pages[i] = phys_to_page(buf->buffer +
 						i * PAGE_SIZE);
 			}
 			ret_value = vmap(pages, npages, VM_IOREMAP, pgprot);
 			vfree(pages);
 		} else {
 			if (ION_IS_CACHED(buffer->flags))
-				ret_value = ioremap_cached(buffer->priv_phys,
+				ret_value = ioremap_cached(buf->buffer,
 							   buffer->size);
 			else
-				ret_value = ioremap(buffer->priv_phys,
+				ret_value = ioremap(buf->buffer,
 						    buffer->size);
 		}
 
@@ -654,10 +602,9 @@
 {
 	struct ion_cp_heap *cp_heap =
 		container_of(heap, struct ion_cp_heap, heap);
+	struct ion_cp_buffer *buf = buffer->priv_virt;
 
-	if (cp_heap->reusable)
-		unmap_kernel_range((unsigned long)buffer->vaddr, buffer->size);
-	else if (cp_heap->cma)
+	if (cp_heap->cma)
 		vunmap(buffer->vaddr);
 	else
 		__arm_iounmap(buffer->vaddr);
@@ -855,8 +862,11 @@
 			const char *client_name = "(null)";
 
 			if (last_end < data->addr) {
-				seq_printf(s, "%16.s %14lx %14lx %14lu (%lx)\n",
-					   "FREE", last_end, data->addr-1,
+				phys_addr_t da;
+
+				da = data->addr-1;
+				seq_printf(s, "%16.s %14pa %14pa %14lu (%lx)\n",
+					   "FREE", &last_end, &da,
 					   data->addr-last_end,
 					   data->addr-last_end);
 			}
@@ -864,9 +874,9 @@
 			if (data->client_name)
 				client_name = data->client_name;
 
-			seq_printf(s, "%16.s %14lx %14lx %14lu (%lx)\n",
-				   client_name, data->addr,
-				   data->addr_end,
+			seq_printf(s, "%16.s %14pa %14pa %14lu (%lx)\n",
+				   client_name, &data->addr,
+				   &data->addr_end,
 				   data->size, data->size);
 			last_end = data->addr_end+1;
 		}
@@ -994,25 +1005,26 @@
 	struct ion_cp_heap *cp_heap =
 		container_of(buffer->heap, struct ion_cp_heap, heap);
 	int prot = IOMMU_WRITE | IOMMU_READ;
+	struct ion_cp_buffer *buf = buffer->priv_virt;
 	prot |= ION_IS_CACHED(flags) ? IOMMU_CACHE : 0;
 
 	data->mapped_size = iova_length;
 
 	if (!msm_use_iommu()) {
-		data->iova_addr = buffer->priv_phys;
+		data->iova_addr = buf->buffer;
 		return 0;
 	}
 
 	if (cp_heap->iommu_iova[domain_num]) {
 		/* Already mapped. */
-		unsigned long offset = buffer->priv_phys - cp_heap->base;
+		unsigned long offset = buf->buffer - cp_heap->base;
 		data->iova_addr = cp_heap->iommu_iova[domain_num] + offset;
 		return 0;
 	} else if (cp_heap->iommu_map_all) {
 		ret = iommu_map_all(domain_num, cp_heap, partition_num, prot);
 		if (!ret) {
 			unsigned long offset =
-					buffer->priv_phys - cp_heap->base;
+					buf->buffer - cp_heap->base;
 			data->iova_addr =
 				cp_heap->iommu_iova[domain_num] + offset;
 			cp_heap->iommu_partition[domain_num] = partition_num;
@@ -1174,8 +1187,8 @@
 		cp_heap->iommu_2x_map_domain =
 				extra_data->iommu_2x_map_domain;
 		cp_heap->cma = extra_data->is_cma;
-		cp_heap->disallow_non_secure_allocation =
-			extra_data->no_nonsecure_alloc;
+		cp_heap->allow_non_secure_allocation =
+			extra_data->allow_nonsecure_alloc;
 
 	}
 
@@ -1225,108 +1238,4 @@
 	*size = cp_heap->total_size;
 }
 
-/*  SCM related code for locking down memory for content protection */
-
-#define SCM_CP_LOCK_CMD_ID	0x1
-#define SCM_CP_PROTECT		0x1
-#define SCM_CP_UNPROTECT	0x0
 
-struct cp_lock_msg {
-	unsigned int start;
-	unsigned int end;
-	unsigned int permission_type;
-	unsigned char lock;
-} __attribute__ ((__packed__));
-
-static int ion_cp_protect_mem_v1(unsigned int phy_base, unsigned int size,
-			      unsigned int permission_type)
-{
-	struct cp_lock_msg cmd;
-	cmd.start = phy_base;
-	cmd.end = phy_base + size;
-	cmd.permission_type = permission_type;
-	cmd.lock = SCM_CP_PROTECT;
-
-	return scm_call(SCM_SVC_CP, SCM_CP_LOCK_CMD_ID,
-			&cmd, sizeof(cmd), NULL, 0);
-}
-
-static int ion_cp_unprotect_mem_v1(unsigned int phy_base, unsigned int size,
-				unsigned int permission_type)
-{
-	struct cp_lock_msg cmd;
-	cmd.start = phy_base;
-	cmd.end = phy_base + size;
-	cmd.permission_type = permission_type;
-	cmd.lock = SCM_CP_UNPROTECT;
-
-	return scm_call(SCM_SVC_CP, SCM_CP_LOCK_CMD_ID,
-			&cmd, sizeof(cmd), NULL, 0);
-}
-
-#define V2_CHUNK_SIZE	SZ_1M
-
-static int ion_cp_change_mem_v2(unsigned int phy_base, unsigned int size,
-			      void *data, int lock)
-{
-	enum cp_mem_usage usage = (enum cp_mem_usage) data;
-	unsigned long *chunk_list;
-	int nchunks;
-	int ret;
-	int i;
-
-	if (usage < 0 || usage >= MAX_USAGE)
-		return -EINVAL;
-
-	if (!IS_ALIGNED(size, V2_CHUNK_SIZE)) {
-		pr_err("%s: heap size is not aligned to %x\n",
-			__func__, V2_CHUNK_SIZE);
-		return -EINVAL;
-	}
-
-	nchunks = size / V2_CHUNK_SIZE;
-
-	chunk_list = allocate_contiguous_ebi(sizeof(unsigned long)*nchunks,
-						SZ_4K, 0);
-	if (!chunk_list)
-		return -ENOMEM;
-
-	for (i = 0; i < nchunks; i++)
-		chunk_list[i] = phy_base + i * V2_CHUNK_SIZE;
-
-	ret = ion_cp_change_chunks_state(memory_pool_node_paddr(chunk_list),
-					nchunks, V2_CHUNK_SIZE, usage, lock);
-
-	free_contiguous_memory(chunk_list);
-	return ret;
-}
-
-static int ion_cp_protect_mem(unsigned int phy_base, unsigned int size,
-			      unsigned int permission_type, int version,
-			      void *data)
-{
-	switch (version) {
-	case ION_CP_V1:
-		return ion_cp_protect_mem_v1(phy_base, size, permission_type);
-	case ION_CP_V2:
-		return ion_cp_change_mem_v2(phy_base, size, data,
-						SCM_CP_PROTECT);
-	default:
-		return -EINVAL;
-	}
-}
-
-static int ion_cp_unprotect_mem(unsigned int phy_base, unsigned int size,
-			      unsigned int permission_type, int version,
-			      void *data)
-{
-	switch (version) {
-	case ION_CP_V1:
-		return ion_cp_unprotect_mem_v1(phy_base, size, permission_type);
-	case ION_CP_V2:
-		return ion_cp_change_mem_v2(phy_base, size, data,
-						SCM_CP_UNPROTECT);
-	default:
-		return -EINVAL;
-	}
-}
